{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bba1344-d01b-4a02-83e7-f32cb0139d47",
   "metadata": {},
   "source": [
    "# Understanding Tokenization with NLTK\n",
    "\n",
    "This notebook explains the concept of **tokenization** â€” the process of splitting text into individual units such as words or sentences â€” using the **NLTK (Natural Language Toolkit)** library in Python.\n",
    "\n",
    "Tokenization is a fundamental step in Natural Language Processing (NLP), used in tasks like text preprocessing, sentiment analysis, and language modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae16c245-3a16-495a-9e61-ff35ca0c9b5b",
   "metadata": {},
   "source": [
    "## Step 1: Importing and Downloading NLTK Tokenizers\n",
    "\n",
    "Before we can tokenize text, we need to import the tokenization tools and download the 'punkt' tokenizer models which are used for sentence and word tokenization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e26d4299-9dee-4222-aa3b-127e1ee92961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jashika/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the necessary modules from NLTK\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Downloading the necessary NLTK data packages (only need to run once)\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7415557f-12f7-4cf3-9e14-287155cae0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77d90b1-aa99-44a1-9116-99b03a3f5377",
   "metadata": {},
   "source": [
    "## Step 2: Define Sample Text\n",
    "\n",
    "We will use a simple English paragraph as our input text for tokenization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "623161ad-5811-4e5a-9563-553a2319ee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus= \"\"\"Hello Welcome,to this Data Science Course.\n",
    "It's a wonderful resource for beginners. It will help you upskill and learn new skills. Keep Visiting Jashika's GitHub Repo. You can also follow Jashika's LinkedIn.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6f0ef1c-4a01-4d27-81c2-929b4e01ade3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Welcome,to this Data Science Course.\n",
      "It's a wonderful resource for beginners. It will help you upskill and learn new skills. Keep Visiting Jashika's GitHub Repo. You can also follow Jashika's LinkedIn.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08252360-16f9-48da-a1ea-9876d2fae4b3",
   "metadata": {},
   "source": [
    "## Step 3: Sentence Tokenization\n",
    "\n",
    "The `sent_tokenize()` function breaks the text into individual sentences using pretrained models. This helps in understanding the structure of paragraphs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a8c8f83-f4f4-4df9-962a-65d5a56922f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Tokenization\n",
    "## Splitting Sentence-->paragraphs\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb945021-e204-4775-b106-72ae4f53e07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3234f77b-dc7a-4cc2-af17-f334275936aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d6256bc-cfa6-4a45-aa07-5a7297eec457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Welcome,to this Data Science Course.\n",
      "It's a wonderful resource for beginners.\n",
      "It will help you upskill and learn new skills.\n",
      "Keep Visiting Jashika's GitHub Repo.\n",
      "You can also follow Jashika's LinkedIn.\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c97ae0c-4c6b-4595-b4c9-aa3bb93111e7",
   "metadata": {},
   "source": [
    "## Step 4: Word Tokenization\n",
    "\n",
    "The `word_tokenize()` function splits the input text into individual words and punctuation marks. This is useful for most NLP tasks such as tagging, parsing, and classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06e8afb0-a6f1-4d13-99ee-e545e7c6081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization \n",
    "## Paragraph-->words\n",
    "## sentence--->words\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e108cab-d882-4ceb-9b0b-beddfe0668c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'this',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'Course',\n",
       " '.',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'wonderful',\n",
       " 'resource',\n",
       " 'for',\n",
       " 'beginners',\n",
       " '.',\n",
       " 'It',\n",
       " 'will',\n",
       " 'help',\n",
       " 'you',\n",
       " 'upskill',\n",
       " 'and',\n",
       " 'learn',\n",
       " 'new',\n",
       " 'skills',\n",
       " '.',\n",
       " 'Keep',\n",
       " 'Visiting',\n",
       " 'Jashika',\n",
       " \"'s\",\n",
       " 'GitHub',\n",
       " 'Repo',\n",
       " '.',\n",
       " 'You',\n",
       " 'can',\n",
       " 'also',\n",
       " 'follow',\n",
       " 'Jashika',\n",
       " \"'s\",\n",
       " 'LinkedIn',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd7a3473-586f-4560-8489-2cf9eec1bad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Welcome', ',', 'to', 'this', 'Data', 'Science', 'Course', '.']\n",
      "['It', \"'s\", 'a', 'wonderful', 'resource', 'for', 'beginners', '.']\n",
      "['It', 'will', 'help', 'you', 'upskill', 'and', 'learn', 'new', 'skills', '.']\n",
      "['Keep', 'Visiting', 'Jashika', \"'s\", 'GitHub', 'Repo', '.']\n",
      "['You', 'can', 'also', 'follow', 'Jashika', \"'s\", 'LinkedIn', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae313d8-6b6a-45be-9ce0-39fdd78795a9",
   "metadata": {},
   "source": [
    "## Comparing `wordpunct_tokenize` vs `TreebankWordTokenizer`\n",
    "\n",
    "In this section, we compare two popular word tokenizers from NLTK:\n",
    "\n",
    "### 1. `wordpunct_tokenize`\n",
    "- Splits text into alphabetic and non-alphabetic characters.\n",
    "- Treats all punctuation as separate tokens.\n",
    "- For example: `\"Don't\"` becomes `[\"Don\", \"'\", \"t\"]`.\n",
    "- Useful for basic splitting but may over-segment text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50b7b103-8437-45a7-94d4-c080c0792654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27251d3b-fa39-4560-999b-a14c73cfef93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'this',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'Course',\n",
       " '.',\n",
       " 'It',\n",
       " \"'\",\n",
       " 's',\n",
       " 'a',\n",
       " 'wonderful',\n",
       " 'resource',\n",
       " 'for',\n",
       " 'beginners',\n",
       " '.',\n",
       " 'It',\n",
       " 'will',\n",
       " 'help',\n",
       " 'you',\n",
       " 'upskill',\n",
       " 'and',\n",
       " 'learn',\n",
       " 'new',\n",
       " 'skills',\n",
       " '.',\n",
       " 'Keep',\n",
       " 'Visiting',\n",
       " 'Jashika',\n",
       " \"'\",\n",
       " 's',\n",
       " 'GitHub',\n",
       " 'Repo',\n",
       " '.',\n",
       " 'You',\n",
       " 'can',\n",
       " 'also',\n",
       " 'follow',\n",
       " 'Jashika',\n",
       " \"'\",\n",
       " 's',\n",
       " 'LinkedIn',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a39e75-8d04-4d20-a355-36def6132a3d",
   "metadata": {},
   "source": [
    "### 2. `TreebankWordTokenizer`\n",
    "- Based on the Penn Treebank conventions.\n",
    "- Smarter with handling contractions, punctuation, and special characters.\n",
    "- For example: `\"Don't\"` becomes `[\"Do\", \"n't\"]`, which reflects actual language usage in NLP models.\n",
    "- Preferred in most NLP pipelines that require contextual understanding.\n",
    "\n",
    "`wordpunct_tokenize` vs `TreebankWordTokenizer`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1ca741a-d56a-4672-8fa9-7a6caee27d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0069a3fc-bdaf-4ff7-973b-7954710e896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22f1a70e-7181-41dc-b777-b13f61ba9820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'this',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'Course.',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'wonderful',\n",
       " 'resource',\n",
       " 'for',\n",
       " 'beginners.',\n",
       " 'It',\n",
       " 'will',\n",
       " 'help',\n",
       " 'you',\n",
       " 'upskill',\n",
       " 'and',\n",
       " 'learn',\n",
       " 'new',\n",
       " 'skills.',\n",
       " 'Keep',\n",
       " 'Visiting',\n",
       " 'Jashika',\n",
       " \"'s\",\n",
       " 'GitHub',\n",
       " 'Repo.',\n",
       " 'You',\n",
       " 'can',\n",
       " 'also',\n",
       " 'follow',\n",
       " 'Jashika',\n",
       " \"'s\",\n",
       " 'LinkedIn',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a8d2e0-9c77-447b-bceb-fbf71cf0df07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b14d1a8-91da-4371-a60c-933f1f5283c4",
   "metadata": {},
   "source": [
    "\n",
    "ðŸ‘‰ Use `TreebankWordTokenizer` for cleaner, linguistically-aware tokenization.  \n",
    "ðŸ‘‰ Use `wordpunct_tokenize` if you need raw punctuation separation.\n",
    "\n",
    "| Feature                  | wordpunct_tokenize                     | TreebankWordTokenizer                 |\n",
    "|--------------------------|----------------------------------------|---------------------------------------|\n",
    "| Punctuation handling     | Splits all punctuation separately      | Keeps common English structure        |\n",
    "| Contraction: \"It's\"      | ['It', \"'\", 's']                        | ['It', \"'s\"]                          |\n",
    "| Possessive: \"Jashika's\"  | ['Jashika', \"'\", 's']                   | ['Jashika', \"'s\"]                     |\n",
    "| Use Case                 | Rule-based, regex-preprocessing        | NLP pipelines like tagging, parsing   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def1e866-8818-459b-b570-b237503879f5",
   "metadata": {},
   "source": [
    "## ðŸ“˜ Summary\n",
    "\n",
    "- **Tokenization** is the process of converting a text into smaller components (tokens).\n",
    "- NLTK provides easy-to-use methods like `sent_tokenize()` for sentence splitting and `word_tokenize()` for word splitting.\n",
    "- Tokenization helps prepare raw text data for further NLP tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
